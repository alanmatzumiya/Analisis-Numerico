
\newpage
\begin{enumerate}
	\item
	Hacer un script de python que reprodusca la tabla y figura 3.1 de la pg. 35
	\item	
		Use the TS(2) method to solve the IVP
		\begin{align*}
			x'(t) = 2x(t)&(1 - x(t)) , t>10 \\
			x&(10) = 1/5,
		\end{align*}
		with $h = 0.5$ and compare the accuracy of the solution at $t = 11$
		with that of Euler’s method using $h = 0.2$ (see Example 2.2).
	\item
		Apply the TS(2) method to the IVP \\
		\begin{align*}
			x'(t) &= 1 + t - x(t) , t>0 \\
			x(0) &= 0
		\end{align*}
		and derive a recurrence formula for determining $x_{n+1}$ in terms of
		$x_n$ , $t_n$ , and $h$. Use this recurrence to calculate $x_1$ , $x_2$ , . . . 
		and deduce an expression for $x_n$ in terms of $n$ and $h$. Show that $x_n = x(t_n )$ for $n = 0, 1, 2, . . . .$ Explain your findings by appealing to the nature of
		the LTE (remainder term) in this case.
	\item
		Derive the TS(2) method for the first-order systems obtained in Exercise 1.5 (a). 
		Use both TS(1) and TS(2) to determine approximate solutions at $t = 0.2$ using $h = 0.1$ .
	\item
		One way of estimating the GE without knowledge of the exact solution is to compute approximate solutions at $t = t_n$ using both $TS(p)$ and $TS(p + 1)$. We will denote these by $x_n^{[p]}$ and $x_n^{[p + 1]}$ , respectively.
		The GE in the lower order method is, by definition,\\
		\begin{align*}
			e_n = x(t_n) - x_n^{[p]}
		\end{align*}
			with $e_n = \mathcal{O}(h^p)$ and, for the higher order method: $x(t_n) - x_n^{[p + 1]} = \mathcal{O}(h^{p + 1})$. Thus,
		\begin{align*}
			e_n = x_n^{[p+1]} - x_n^{[p]} + \mathcal{O}(h^{p + 1})
		\end{align*}
		from which it follows that the leading term in the GE of the lower order method may be estimated by the difference in the two computed solutions.\\
		Use this process on the data in the first three columns of Table 3.1
		and compare with the actual GE for Euler’s method given in the
		fourth column.
	\item
		Apply Euler’s method to the IVP $x'(t) = \lambda x(t)$, $x(0) = 1$, with a
		step size h. Assuming that $\lambda$ is a real number:
		\item[a)]
		What further condition is required on $\lambda$ to ensure that the solution $x(t) \longrightarrow 0$ as $t \longrightarrow \infty$?\\
		\\
		La solucion exacta es $x(t) = e^{\lambda t}$ y por lo tanto $\lambda < 0$ para que
		$x(t) \longrightarrow 0$ cuando $t \longrightarrow \infty$
		\item[b)]
		What condition on $h$ then ensures that $|x_n | \longrightarrow 0$ as $n \longrightarrow \infty$?\\
		Compare the cases where $\lambda = -1$ and $\lambda = -100$.\\
		What is the corresponding condition if TS(2) is used instead of Euler’s method?\\
		\\
		Aplicando el metodo de Euler se tiene $x_{n+1} = x_n + h\lambda x_n = (1+h\lambda)x_n$, como $x_0 = 1$, se deduce que $x_n = (1+h\lambda)^n$.Entonces $|x_n | \longrightarrow 0$ si y solo si $|1 + h\lambda| < 1$\\
		Por lo tanto se obtien la siguiente desigualad:
		\begin{align*}
			-1 < 1 + h\lambda < 1 \Longleftrightarrow -2 < h\lambda < 0
		\end{align*}
		Si $\lambda = -1$, se requiere que $0 < h < 2$	, y si $\lambda = -100$ el rango de $h$ es $0 < h < 0.02$\\
		\\
		Si aplicamos TS(2) se obtiene que $x_{n+1} = (1 + h\lambda + \frac{1}{2} h^2 \lambda^2)x_n$. Como $x_0 = 1$, se deduce que $x_n = (1 + h\lambda + \frac{1}{2} h^2 \lambda^2)^n$. Entonces  $|x_n | \longrightarrow 0$ si y solo si $|(1 + h\lambda + \frac{1}{2} h^2 \lambda^2)| < 1$\\ 
		Por lo tanto se tiene la siguiente desigualdad:
		\begin{align*}
			-1 <(1 + h\lambda + \frac{1}{2} h^2 \lambda^2) < 1
		\end{align*}
		Como $1 + h\lambda + \frac{1}{2} h^2 \lambda^2 = \frac{1}{2}(1 + h\lambda)^2 + \frac{1}{2}$, se tiene que $-3 < (1 + h\lambda)^2 < 1$ y asi $-4 < h\lambda(2 + h\lambda) < 0$.
		Por lo tanto se deduce que $-2 < h\lambda < 0$ y asi $h$ tendra las mismas condiciones que en el metodo de euler.
	\item
		Write down the TS(3) method for the IVP $x'(t) = \lambda x(t)$, $x(0) = 1$.
		Repeat for the IVP in Example 3.1.
	\item
		A rough estimate of the effort required in evaluating a formula may be obtained by counting the number of arithmetic operations $(+, -, \times, \div)$ — this is known as the number of $flops$ (floating point operations). For example, the calculation of $3 + 4/5^2$ needs three flops. These may be used as a basis for comparing different methods.\\
		What are the flop counts per step for the TS(p) methods for the IVP
		in Example 3.1 for $p = 1, 2, 3$?\\
		How many flops do you estimate would be required by TS(1) and TS(2) in Example 3.1 to achieve a GE of $0.01$? (Use the data provided in Table 3.1.) Comment on your answer.\\
		\\
		\\
		En el TS(1), para cada $n$ se requieren 1 flop para $t_n$, 3 flops para $x'(t_n) = (1-2t_n)x(t_n)$ y 2 flops para $x_{n+1} = x_n + hx_n'$. En total son 6 flops para cada $n$.\\
		\\
		En el TS(2), se requieren los mismos que en el TS(1) solo que agregando los calculos para $x''(t_n) = [(1 - 2t_n)^2 - 2]x(t_n) $ que son 3 flops y ademas para $\frac{1}{2} h^2x_n''$ son otros 2 flops, asi que en total son 11 flops para cada $n$.\\
		\\
		Para TS(3), para $x'''(t_n) = [(1 - 2t_n)^3 - 6]x(t_n) $  son 3 flops y para $\frac{1}{6} h^2x_n'''$ son otros 2 flops mas, en total serian 16 flops para cada $n$.\\
		\\
		En base al problema del ejemplo 3.1, tenemos que para TS(1) el $GE \approx 0.77h$, entonces para obtener $GE = 0.01$ se requiere que $h \approx 0.013$. Entonces para $t = 1.2$ se requieren aproximadamente 93 pasos para obtener este valor de GE, y como para cada paso se requieren 6 flops, el numero total de flops que se requieren son aproximadamente $6 \times 93 = 558$ flops.\\
		Para TS(2), $GE \approx 0.14h^2$. Entonces para GE = 0.01 se requiere que h = 0.27 y asi, para t = 1.2 se necesitan 5 pasos. Por lo tanto se requieren $11 \times 5  = 55$ flops en total.
		
	\item
		For the IVP
		\begin{align*}
			u'(t) = v&(t), v'(t) = -u(t), t > 0 \\
			u&(0) = 1, v(0) = 0, 
		\end{align*}
	    use the chain rule to differentiate $u^2(t) + v^2(t)$ with respect to t.
		Hence prove that $u^2(t) + v^2(t) = 1$ for all $t \geq 0$.\\
		\\
		Aplicando la regla de la cadena se tiene que:
		\begin{align*}
			\frac{d}{dt}(u^2(t) + v^2(t)) = 2u(t)u'(t) + 2v(t)v'(t) = 2u(t)v(t) - 2u(t)v(t) = 0
		\end{align*}
		Como la derivada es cero para toda $t \geq 0$ entonces $u^2(t) + v^2(t)$ es constante y en particular para $t = 0$ se debe cumplir que $u^2(0) + v^2(0) = 1$.
		Por lo tanto $u^2(t) + v^2(t) = 1$ para toda $t\geq0$\\
		\\
		Use the TS(2) method to derive a means of computing $u_{n+1}$ and
		$v_{n+1}$ in terms of $u_n$ and $v_n$ .\\
		
		Prove that this TS(2) approximation satisfies $u_n^2 + v_n^2 = (1 + \frac{1}{4}h^4)^n$ when $u_0 = 1$ and $v_0 = 0$.
		\\
		Aplicando TS2 para el sistema tenemos:\\
		\begin{align*}
			u_{n+1} &= u_n + hu_n' + \frac{1}{2}hu_n'' = u_n + h^2v_n - \frac{1}{2}h^2v_n\\
			v_{n+1} &= v_n + hv_n' + \frac{1}{2}hv_n'' = v_n - h^2u_n - \frac{1}{2}h^2v_n
		\end{align*}
		Si elevamos al cuadrado cada ecuacion se tiene:
		\begin{align*}
			u_{n+1}^2 &= u_n^2 + 2hu_nv_n + h^2v_n^2 - h^2u_n^2 - h^3u_nv_n + \frac{1}{4}h^4u_n^2 \\
			v_{n+1}^2 &= v_n^2 - 2hu_nv_n + h^2u_n^2 - h^2v_n^2 + h^3u_nv_n + \frac{1}{4}h^4v_n^2
		\end{align*}
		Sumando las dos ecuaciones anteriores:
		\begin{align*}
			u_{n+1}^2 + v_{n+1}^2 = u_n^2 + v_n^2 + \frac{1}{4}h^4(u_n^2 + v_n^2)
		\end{align*}
		Si hacemos $w_{n+1} = u_{n+1}^2 + v_{n+1}^2$ y $w_n = u_n^2 + v_n^2$ y cambiando 
		la condicion inicial como $w(0) = u(0) + v(0) = 1$,  se tiene:
		\begin{align*}
			w_{n+1} = w_n + \frac{1}{4}h^4(w_n) = w_n(1 + \frac{1}{4}h^4) 
		\end{align*}
		y entonces de lo anterior se obtiene con las condiciones iniciales dadas que
		\begin{align*}
			u_{n}^2 + v_{n}^2 = w_n = (1 + \frac{1}{4}h^4)^n
		\end{align*}			
	\item
		If $x'(t) = f (t, x(t))$ , show that
		\begin{align*}
			x''(t) = f_t(t, x) + f (t, x)f_x(t, x),
		\end{align*}
			where $f_t$ and $f_x$ are the partial derivatives of $f(t, x)$.\\
			\\
			Aplicando la regla de la cadena
		\begin{align*}
			x''(t) = \frac{d}{dt}x'(t) = \frac{d}{dt}f(t, x(t)) = f_t(t, x) + f_x(t, x)x'(t) = f_t(t, x) + f (t, x)f_x(t, x)
		\end{align*}		
	\item
		Prove that
		\begin{align*}
			e^x \geq 1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{p!} x^p
		\end{align*}
		for all $x \geq 0$. [Hint: use induction starting with the case given in
		Exercise 2.9 and integrate to move from one induction step to the
		next.]\\
		Aplicando induccion sobre $p$ \\
		Para $p = 1$ ya esta probado, es el caso del problema 2.9, es decir \\
		\begin{align*}
			e^x \geq 1 + x 
		\end{align*}
		supongamos que para $p = k$ se cumple la desigualdad, es decir:
		\begin{align*}
			e^x \geq 1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{k!} x^k
		\end{align*}
		Como en ambos lados tenemos dos funciones integrables en el intervalo $0 \leq t \leq x$
		\[
		\int_{0}^{x} e^{t} \, dx \geq \int_{0}^{x} (1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{k!} x^k) \, dx  
		\]
		\begin{align*}
			\Longrightarrow e^x - 1 \geq x + \frac{1}{2!} x^2 + ... + \frac{1}{k!} x^k + \frac{1}{(k+1)!} x^{k+1} \\
			\Longrightarrow e^x \geq 1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{k!} x^k + \frac{1}{(k+1)!} x^{k+1}
		\end{align*}		
	\item
		When $x(t)$ denotes the solution of the IVP in Theorem 3.2, the
		first $p$ derivatives required for the Taylor expansion (3.4) of $x(t_n + h)$
		may be obtained by repeated differentiation of the ODE:
		\begin{align*}
			x'&(t_n) = \lambda x(t_n) + g(t_n),\\
			x''&(t_n) = \lambda x'(t_n) + g'(t_n),\\
			.\\
			.\\
			.\\	
			x^{(p)}&(t_n) = \lambda x^{(p-1)}(t_n) + g^{(p-1)}(t_n).	
		\end{align*}
		By using analogous relationships between approximations of the derivatives
		 $(x_n^{(j+1)} = \lambda x_n^j + g^j (t_n)$ for the $(j + 1)th$ derivative,
		for example) show that the GE for the TS(p) method (3.5) for the
		same IVP satisfies
		\begin{align*}
			e_{n+1} = r(\lambda h)e_n + T_{n+1},   n = 0, 1, 2, . . . ,
		\end{align*}
		where $r(s) = 1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{p!} x^p$ , the first $(p + 1)$ terms in $s$ the Maclaurin expansion of $e$ , and $T_{n+1} = \mathcal{O}(h^{p+1})$ [compare with Equation (2.15)].\\
		Hence complete the proof of Theorem 3.2.
		\begin{theorem}
			Euler's method applied to the IVP
			\begin{align*}
			x'(t) & = \lambda x(t) + g(t), \qquad 0<t\leq t_f,\\
			x(0) & = 1,
			\end{align*}
			where $\lambda \in \mathbb{C}$ and $g$ is a continuously differentiable function
			, converges and the GE at any $t \in [0, t_f]$ is $\mathcal{O}(h)$\\
			\\
			Demostracion. Diferenciando la ecuacion $x'(t) = \lambda x(t) + g(t)$ hasta la derivada p y evualamos para $t = t_n$, se obtiene el siguiente sistema:
			\begin{align*}
			x'&(t_n) = \lambda x(t_n) + g(t_n),\\
			x''&(t_n) = \lambda x'(t_n) + g'(t_n),\\
			.\\
			.\\
			.\\	
			x^{(p)}&(t_n) = \lambda x^{(p-1)}(t_n) + g^{(p-1)}(t_n).\\
			\end{align*}
			Asi, aproximando cada una de estas ecuaciones se obtiene lo siguiente:
			\begin{align*}
			x'& = \lambda x_n + g(t_n),\\
			x''& = \lambda x' + g'(t_n),\\
			.\\
			.\\
			.\\	
			x^(p)& = \lambda x^{(p-1)} + g^{(p-1)}(t_n).	
			\end{align*}
			Si restamos cada una de las ecuaciones en su repectivo orden se obtiene la siguiente expresion.
			\begin{align*}
				x_n^{(j)}(t_n) - x_n^{(j)} = \lambda^j(x(t_n) - x')
			\end{align*}
			De la ecuacion 3.4 tenemos que para $t = t_n$
			\begin{align*}
			x(t_{n+1}) = x(t_n) + hx'(t_n) + \frac{1}{2!}h^2x''(t_n) + ... + \frac{1}{p!}h^p x^{(p)}(t_n) + T_{n+1}
			\end{align*}
			Donde $T_{n+1} = \mathcal{O}(h^{p+1})$ y ademas el TS(p) para esta ecuacion toma la siguiente expresion:
			\begin{align*}
			x_{n+1} = x_n + hx_n' + \frac{1}{2!}h^2x_n'' + ... + \frac{1}{p!}h^px_n^{(p)}
			\end{align*}
			Restando las dos ecuaciones anteriores obtenemos la siguiente expresion:
			\begin{align*}
			e_{n+1} = r(\lambda h)e_n + T_{n+1}
			\end{align*}
			Donde  $e_n = x(t_n ) - x_n$ y $r(s) = 1 + x + \frac{1}{2!} x^2 + ... + \frac{1}{p!} x^p$. Asi, podemos escribir la siguiente sumatoria:
			\[
			e_n = \sum_{j = 1}^{n}r(\lambda h)^{n-j}T_{j}
			\]
			Del ejercicio 3.10 tenemos que $e^s \geq r(s)$ para $s\geq 0$. Por lo tanto tenemos la siguiente desigualdad:
			\begin{align*}
			|r(\lambda h)^{n-j}| = |r(\lambda h)|^{n-j}\leq r(|\lambda| h)^{n-j} \leq e^{(n-j)|\lambda|h} \leq e^{|\lambda|t_f}
			\end{align*}
			De esta expresion podemos deducir que,\\
			\[
			|e_n| \leq |\sum_{j = 1}^{n}r(\lambda h)^{n-j}T_{j}| \leq e^{|\lambda|t_f}\sum_{j = 1}^{n}|T_{j}|
			\]
			Como $|T_{j}|\leq Ch^{p+1}$ para alguna constante $C$ independiente de $h$ y $j$, se tiene que:
			\begin{align*}
			|e_n| \leq e^{|\lambda|t_f}nCh^{p+1} \leq Ch^pt_fe^{|\lambda|t_f}
			\end{align*}
			Por lo tanto el metodo converge si $p > 0$, ya que $|e_n| \longrightarrow 0$ cuando $h \longrightarrow 0$ para algun $t_n\in (0, t_f)$, y ademas converge a una
			velocidad de orden p
			
			
			
		
			
			
		\end{theorem}


\end{enumerate}
	